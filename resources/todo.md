## Engineering 
* [ ] Add Inflection-1 performance, waiting for its API
* [ ] Add Vicuna 30B performance
* [ ] Instructions about how to contribute to this repo
* [ ] Recommended Prompts
* [ ] Stablize BBH answer extraction
  * [ ] Maybe report answer extraction rate 
* [ ] GPT-3.5 results for each STL 
* [ ] Make this prompt and the associating prompts as a huggingface dataset
* [ ] Detailed raw results in a shared google sheet
* [ ] Random baseline
* [ ] More models
  * [ ] Galactica
  * [x] text-bison-001 and chat-bison-001
  * [ ] Bard -- but how to get the online Bard API? Check how SummEdits do it 
  * [ ] Cohere
  * [ ] AI21Labs
* [ ] More reasoning datasets
  * [ ] Arc-c
  * [ ] Commonsense
  * [ ] [BigCode eval](https://github.com/bigcode-project/bigcode-evaluation-harness)
  * [x] Theorem proving 
  * [x] Coding 
  * [ ] API Call
    * [API Bank](https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/api-bank)
    * [ToolBench](https://github.com/OpenBMB/ToolBench)
  * [ ] proofbank, entailment bank -- maybe sweep all Aristo datasets then see how LLM works 
* [x] Draw a figure about model scale v.s. reasoning accuracy 
* [x] Add Alpaca and Vacuna 
* [ ] Test LLaMA on BBH
* [ ] Fancy tricks in prompt engineering 
* [x] Add smaller LLaMA
* [x] Add Flan-T5

## Research
* [ ] Online active in-context learning from AI feedback 
* [ ] Sensitivity analysis of CoT prompting
  * [ ] Zero shot v.s. One shot v.s. Few shot
  * [ ] Using logits v.s. using string matching 
  * [ ] Greedy v.s. sampling, temperature 
  * [ ] Engineering of answer extraction and string matching 
* [ ] Decoding space 
  * [ ] Do larger models have "larger" decoding space than smaller models? 
  * [ ] Can instruction finetuning "closes" some meaningful/ reasonable decoding path? Can it "open" new decoding paths? 
* [ ] Advanced prompt engineering
  * [ ] Planning and deductive prompting 
  * [x] Dialog in-context learning
* [ ] CoT prompt engineering documentation, including 
  * Stable: 
    * [ ] complexity based prompting
  * Test: 
    * [ ] concise prompting
    * [ ] emphasizing important steps 